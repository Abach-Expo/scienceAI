/**
 * üîç REAL PLAGIARISM SERVICE v2.0
 * Multi-engine plagiarism detection with real web search
 * 
 * Engines:
 * 1. Google Custom Search API ‚Äî real web duplicate detection
 * 2. CrossRef API ‚Äî academic paper matching
 * 3. Semantic Scholar API ‚Äî scientific literature check
 * 4. Internal AI analysis ‚Äî pattern + statistical analysis
 */

import axios from 'axios';
import { logger } from '../utils/logger';

// ================== TYPES ==================

export interface PlagiarismSource {
  url: string;
  title: string;
  snippet: string;
  similarity: number; // 0-100
  type: 'web' | 'academic' | 'journal' | 'book';
}

export interface PlagiarismIssue {
  type: 'plagiarism' | 'ai_pattern' | 'cliche' | 'self_plagiarism';
  text: string;
  suggestion: string;
  severity: 'low' | 'medium' | 'high';
  source?: PlagiarismSource;
}

export interface PlagiarismResult {
  uniquenessScore: number;      // 0-100
  aiProbability: number;        // 0-100
  sourcesFound: number;
  sources: PlagiarismSource[];
  analysis: {
    issues: PlagiarismIssue[];
    strengths: string[];
    summary: string;
  };
  recommendations: string[];
  wordCount: number;
  characterCount: number;
  sentenceCount: number;
  // Detailed metrics
  metrics: {
    exactMatchPercent: number;
    paraphraseMatchPercent: number;
    citationCoverage: number;
    vocabularyRichness: number;
    burstiness: number;
    perplexity: number;
  };
}

// ================== AI PATTERNS DATABASE ==================

const AI_PATTERNS_RU: Array<{ pattern: RegExp; weight: number; category: string }> = [
  // –í–≤–µ–¥–µ–Ω–∏–µ / Opening clich√©s
  { pattern: /–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /–≤ –Ω–∞—à–µ –≤—Ä–µ–º—è/gi, weight: 2, category: 'intro_cliche' },
  { pattern: /–≤ —ç–ø–æ—Ö—É —Ü–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏–∏/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /–¥–∞–Ω–Ω–∞—è —Ç–µ–º–∞.{0,30}–∞–∫—Ç—É–∞–ª—å–Ω/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.{0,30}–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞/gi, weight: 2, category: 'intro_cliche' },
  { pattern: /–Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å/gi, weight: 2, category: 'intro_cliche' },
  { pattern: /–≤ —É—Å–ª–æ–≤–∏—è—Ö –≥–ª–æ–±–∞–ª–∏–∑–∞—Ü–∏–∏/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–µ–∞–ª–∏–π/gi, weight: 3, category: 'intro_cliche' },
  
  // Transitions / –°–≤—è–∑—É—é—â–∏–µ
  { pattern: /–≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ/gi, weight: 2, category: 'transition' },
  { pattern: /—Å–ª–µ–¥—É–µ—Ç –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å/gi, weight: 1, category: 'transition' },
  { pattern: /–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—á–∏—Ç—ã–≤–∞—Ç—å/gi, weight: 1, category: 'transition' },
  { pattern: /—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å/gi, weight: 1, category: 'transition' },
  { pattern: /–∫—Ä–æ–º–µ —Ç–æ–≥–æ/gi, weight: 1, category: 'transition' },
  { pattern: /–±–æ–ª–µ–µ —Ç–æ–≥–æ/gi, weight: 2, category: 'transition' },
  { pattern: /–ø–æ–º–∏–º–æ —ç—Ç–æ–≥–æ/gi, weight: 1, category: 'transition' },
  { pattern: /–≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è/gi, weight: 1, category: 'transition' },
  { pattern: /—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã/gi, weight: 1, category: 'transition' },
  { pattern: /–≤–º–µ—Å—Ç–µ —Å —Ç–µ–º/gi, weight: 1, category: 'transition' },
  
  // Conclusions / –ó–∞–∫–ª—é—á–µ–Ω–∏—è
  { pattern: /—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥/gi, weight: 3, category: 'conclusion' },
  { pattern: /–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç —Å–∫–∞–∑–∞—Ç—å/gi, weight: 3, category: 'conclusion' },
  { pattern: /—Ä–µ–∑—é–º–∏—Ä—É—è –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–µ/gi, weight: 3, category: 'conclusion' },
  { pattern: /–ø–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥/gi, weight: 2, category: 'conclusion' },
  { pattern: /–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –≤—ã—à–µ–∏–∑–ª–æ–∂–µ–Ω–Ω–æ–≥–æ/gi, weight: 2, category: 'conclusion' },
  { pattern: /–∏—Å—Ö–æ–¥—è –∏–∑ –≤—ã—à–µ—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ/gi, weight: 2, category: 'conclusion' },
  
  // Amplifiers / –£—Å–∏–ª–∏—Ç–µ–ª–∏
  { pattern: /–Ω–µ –ø–æ–¥–ª–µ–∂–∏—Ç —Å–æ–º–Ω–µ–Ω–∏—é/gi, weight: 2, category: 'amplifier' },
  { pattern: /–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ/gi, weight: 2, category: 'amplifier' },
  { pattern: /–±–µ–∑—É—Å–ª–æ–≤–Ω–æ/gi, weight: 2, category: 'amplifier' },
  { pattern: /–æ—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ/gi, weight: 1, category: 'amplifier' },
  { pattern: /–Ω–µ–æ—Å–ø–æ—Ä–∏–º—ã–º —è–≤–ª—è–µ—Ç—Å—è/gi, weight: 3, category: 'amplifier' },
  
  // AI-specific patterns
  { pattern: /—è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å/gi, weight: 2, category: 'ai_specific' },
  { pattern: /—à–∏—Ä–æ–∫–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–ø–æ –º–Ω–µ–Ω–∏—é –º–Ω–æ–≥–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤/gi, weight: 3, category: 'ai_specific' },
  { pattern: /–º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç/gi, weight: 3, category: 'ai_specific' },
  { pattern: /–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ/gi, weight: 2, category: 'ai_specific' },
  { pattern: /—è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–∑–∞–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–æ–µ –º–µ—Å—Ç–æ/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç –≤—Å—ë –±–æ–ª—å—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ/gi, weight: 2, category: 'ai_specific' },
  { pattern: /—Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞/gi, weight: 2, category: 'ai_specific' },
  { pattern: /—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è —Ä—è–¥–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–µ—Å/gi, weight: 2, category: 'ai_specific' },
  { pattern: /–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ —Ä—è–¥–æ–º —Ñ–∞–∫—Ç–æ—Ä–æ–≤/gi, weight: 2, category: 'ai_specific' },
  
  // Repetitive structures
  { pattern: /–≤–æ-–ø–µ—Ä–≤—ã—Ö.{20,300}–≤–æ-–≤—Ç–æ—Ä—ã—Ö.{20,300}–≤-—Ç—Ä–µ—Ç—å–∏—Ö/gis, weight: 2, category: 'structure' },
  { pattern: /—Å –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã.{20,300}—Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã/gis, weight: 1, category: 'structure' },
];

const AI_PATTERNS_EN: Array<{ pattern: RegExp; weight: number; category: string }> = [
  { pattern: /in today's world/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /in this day and age/gi, weight: 3, category: 'intro_cliche' },
  { pattern: /it is important to note/gi, weight: 2, category: 'transition' },
  { pattern: /it goes without saying/gi, weight: 3, category: 'amplifier' },
  { pattern: /furthermore/gi, weight: 2, category: 'transition' },
  { pattern: /moreover/gi, weight: 2, category: 'transition' },
  { pattern: /additionally/gi, weight: 2, category: 'transition' },
  { pattern: /in conclusion/gi, weight: 1, category: 'conclusion' },
  { pattern: /plays a crucial role/gi, weight: 2, category: 'ai_specific' },
  { pattern: /has become increasingly/gi, weight: 1, category: 'ai_specific' },
  { pattern: /it is worth mentioning/gi, weight: 2, category: 'ai_specific' },
  { pattern: /delve into/gi, weight: 3, category: 'ai_specific' },
  { pattern: /leverage/gi, weight: 2, category: 'ai_specific' },
  { pattern: /multifaceted/gi, weight: 2, category: 'ai_specific' },
  { pattern: /holistic approach/gi, weight: 3, category: 'ai_specific' },
  { pattern: /paradigm shift/gi, weight: 3, category: 'ai_specific' },
  { pattern: /robust framework/gi, weight: 3, category: 'ai_specific' },
  { pattern: /myriad of/gi, weight: 2, category: 'ai_specific' },
  { pattern: /tapestry of/gi, weight: 3, category: 'ai_specific' },
  { pattern: /landscape of/gi, weight: 2, category: 'ai_specific' },
  { pattern: /underscore the importance/gi, weight: 2, category: 'ai_specific' },
  { pattern: /navigating the complexities/gi, weight: 3, category: 'ai_specific' },
  { pattern: /fostering a sense of/gi, weight: 3, category: 'ai_specific' },
  { pattern: /pivotal role/gi, weight: 2, category: 'ai_specific' },
  { pattern: /cornerstone of/gi, weight: 2, category: 'ai_specific' },
];

// ================== TEXT ANALYSIS ==================

function analyzeTextStatistics(text: string) {
  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 5);
  const words = text.split(/\s+/).filter(w => w.length > 0);
  const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 20);
  
  // Sentence length variation (burstiness)
  const sentenceLengths = sentences.map(s => s.split(/\s+/).length);
  const avgSentenceLen = sentenceLengths.reduce((a, b) => a + b, 0) / sentenceLengths.length || 0;
  const sentenceVariance = sentenceLengths.reduce((sum, len) => sum + Math.pow(len - avgSentenceLen, 2), 0) / sentenceLengths.length || 0;
  const sentenceStdDev = Math.sqrt(sentenceVariance);
  const burstiness = avgSentenceLen > 0 ? sentenceStdDev / avgSentenceLen : 0;
  
  // Vocabulary richness (TTR - Type-Token Ratio)
  const uniqueWords = new Set(words.map(w => w.toLowerCase().replace(/[^–∞-—è—ëa-z]/gi, '')).filter(w => w.length > 2));
  const ttr = words.length > 0 ? uniqueWords.size / words.length : 0;
  
  // Hapax legomena (words appearing only once)
  const wordFreq: Record<string, number> = {};
  words.forEach(w => {
    const normalized = w.toLowerCase().replace(/[^–∞-—è—ëa-z]/gi, '');
    if (normalized.length > 2) wordFreq[normalized] = (wordFreq[normalized] || 0) + 1;
  });
  const hapaxCount = Object.values(wordFreq).filter(c => c === 1).length;
  const hapaxRatio = uniqueWords.size > 0 ? hapaxCount / uniqueWords.size : 0;
  
  // Paragraph length variation
  const paragraphLengths = paragraphs.map(p => p.split(/\s+/).length);
  const avgParaLen = paragraphLengths.reduce((a, b) => a + b, 0) / paragraphLengths.length || 0;
  const paraVariance = paragraphLengths.reduce((sum, len) => sum + Math.pow(len - avgParaLen, 2), 0) / paragraphLengths.length || 0;
  const paraStdDev = Math.sqrt(paraVariance);
  
  // Perplexity approximation (word predictability)
  const bigrams: Record<string, number> = {};
  const unigramTotal: Record<string, number> = {};
  for (let i = 0; i < words.length - 1; i++) {
    const w1 = words[i].toLowerCase();
    const w2 = words[i + 1].toLowerCase();
    const bigram = `${w1} ${w2}`;
    bigrams[bigram] = (bigrams[bigram] || 0) + 1;
    unigramTotal[w1] = (unigramTotal[w1] || 0) + 1;
  }
  let logProb = 0;
  let bigramCount = 0;
  for (const [bigram, count] of Object.entries(bigrams)) {
    const w1 = bigram.split(' ')[0];
    const prob = count / (unigramTotal[w1] || 1);
    logProb += Math.log2(prob);
    bigramCount++;
  }
  const perplexity = bigramCount > 0 ? Math.pow(2, -logProb / bigramCount) : 0;
  
  return {
    wordCount: words.length,
    sentenceCount: sentences.length,
    paragraphCount: paragraphs.length,
    avgSentenceLength: avgSentenceLen,
    sentenceStdDev,
    burstiness,
    ttr,
    hapaxRatio,
    paragraphVariance: paraStdDev,
    perplexity,
    characterCount: text.length,
  };
}

function detectAIPatterns(text: string, language: 'ru' | 'en' = 'ru'): PlagiarismIssue[] {
  const patterns = language === 'ru' ? AI_PATTERNS_RU : AI_PATTERNS_EN;
  const issues: PlagiarismIssue[] = [];
  
  for (const { pattern, weight, category } of patterns) {
    const matches = text.match(pattern);
    if (matches) {
      for (const match of matches) {
        const severity = weight >= 3 ? 'high' : weight >= 2 ? 'medium' : 'low';
        issues.push({
          type: 'ai_pattern',
          text: match,
          suggestion: getPatternFix(match, category, language),
          severity,
        });
      }
    }
  }
  
  return issues;
}

function getPatternFix(text: string, category: string, language: string): string {
  const fixes: Record<string, Record<string, string[]>> = {
    ru: {
      intro_cliche: ['–°–µ–≥–æ–¥–Ω—è', '–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è', '–ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç', '–° —É—á—ë—Ç–æ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π'],
      transition: ['–ü—Ä–∏ —ç—Ç–æ–º', '–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ', '–ù–∞—Ä—è–¥—É —Å —ç—Ç–∏–º', '–û—Ç–º–µ—Ç–∏–º —Ç–∞–∫–∂–µ'],
      conclusion: ['–û–±–æ–±—â–∞—è —Å–∫–∞–∑–∞–Ω–Ω–æ–µ', '–í –∏—Ç–æ–≥–µ', '–°—É–º–º–∏—Ä—É—è', '–ò—Ç–∞–∫'],
      amplifier: ['–í–µ—Ä–æ—è—Ç–Ω–æ', '–ü–æ –≤—Å–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏', '–ö–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è', '–° –≤—ã—Å–æ–∫–æ–π –¥–æ–ª–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏'],
      ai_specific: ['(–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏)', '–ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –≤–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞'],
      structure: ['–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏'],
    },
    en: {
      intro_cliche: ['Currently', 'Today', 'At present', 'As of now'],
      transition: ['Meanwhile', 'Also', 'In addition', 'Similarly'],
      conclusion: ['In summary', 'Overall', 'To sum up', 'All in all'],
      amplifier: ['Likely', 'Presumably', 'Arguably', 'It appears that'],
      ai_specific: ['(rephrase in your own words)', 'Replace with a specific example'],
      structure: ['Use numbered list or subheadings'],
    },
  };
  
  const langFixes = fixes[language] || fixes['ru'];
  const categoryFixes = langFixes[category] || langFixes['ai_specific'];
  return categoryFixes[Math.floor(Math.random() * categoryFixes.length)];
}

// ================== WEB SEARCH ENGINE ==================

async function searchWebForDuplicates(textChunks: string[]): Promise<PlagiarismSource[]> {
  const sources: PlagiarismSource[] = [];
  
  // Use Google Custom Search API if configured
  const googleApiKey = process.env.GOOGLE_SEARCH_API_KEY;
  const googleCx = process.env.GOOGLE_SEARCH_CX;
  
  if (googleApiKey && googleCx) {
    try {
      for (const chunk of textChunks.slice(0, 5)) { // Limit to 5 searches
        // Take most distinctive sentence from chunk
        const query = extractSearchQuery(chunk);
        if (!query || query.length < 20) continue;
        
        const response = await axios.get('https://www.googleapis.com/customsearch/v1', {
          params: {
            key: googleApiKey,
            cx: googleCx,
            q: `"${query}"`,
            num: 3,
          },
          timeout: 8000,
        });
        
        if (response.data.items) {
          for (const item of response.data.items) {
            const similarity = calculateTextSimilarity(chunk, item.snippet || '');
            if (similarity > 25) {
              sources.push({
                url: item.link,
                title: item.title,
                snippet: item.snippet || '',
                similarity,
                type: 'web',
              });
            }
          }
        }
        
        // Rate limit: small delay between searches
        await new Promise(r => setTimeout(r, 300));
      }
    } catch (error) {
      logger.error('[Plagiarism] Google search error:', error);
    }
  }
  
  return sources;
}

async function searchAcademicSources(textChunks: string[]): Promise<PlagiarismSource[]> {
  const sources: PlagiarismSource[] = [];
  
  try {
    for (const chunk of textChunks.slice(0, 3)) {
      const query = extractSearchQuery(chunk);
      if (!query || query.length < 15) continue;
      
      // CrossRef API (free, no key needed)
      try {
        const crossrefResponse = await axios.get('https://api.crossref.org/works', {
          params: {
            query: query.substring(0, 150),
            rows: 3,
            select: 'title,DOI,URL,abstract',
          },
          timeout: 8000,
          headers: { 'User-Agent': 'ScienceAI/1.0 (mailto:support@science-ai.app)' },
        });
        
        if (crossrefResponse.data?.message?.items) {
          for (const item of crossrefResponse.data.message.items) {
            const title = Array.isArray(item.title) ? item.title[0] : item.title || '';
            const abstract = item.abstract || '';
            const combined = `${title} ${abstract}`;
            const similarity = calculateTextSimilarity(chunk, combined);
            
            if (similarity > 20) {
              sources.push({
                url: item.URL || `https://doi.org/${item.DOI}`,
                title,
                snippet: abstract.substring(0, 200),
                similarity,
                type: 'academic',
              });
            }
          }
        }
      } catch (e) {
        // CrossRef may fail silently
      }
      
      // Semantic Scholar API (free)
      try {
        const ssResponse = await axios.get('https://api.semanticscholar.org/graph/v1/paper/search', {
          params: {
            query: query.substring(0, 100),
            limit: 3,
            fields: 'title,abstract,url,externalIds',
          },
          timeout: 8000,
        });
        
        if (ssResponse.data?.data) {
          for (const paper of ssResponse.data.data) {
            const combined = `${paper.title || ''} ${paper.abstract || ''}`;
            const similarity = calculateTextSimilarity(chunk, combined);
            
            if (similarity > 20) {
              sources.push({
                url: paper.url || `https://www.semanticscholar.org/paper/${paper.paperId}`,
                title: paper.title || '',
                snippet: (paper.abstract || '').substring(0, 200),
                similarity,
                type: 'academic',
              });
            }
          }
        }
      } catch (e) {
        // SS may fail silently
      }
      
      await new Promise(r => setTimeout(r, 500));
    }
  } catch (error) {
    logger.error('[Plagiarism] Academic search error:', error);
  }
  
  return sources;
}

// ================== TEXT PROCESSING HELPERS ==================

function extractSearchQuery(text: string): string {
  // Pick the longest, most unique sentence from the chunk
  const sentences = text.split(/[.!?]+/).map(s => s.trim()).filter(s => s.length > 30);
  if (sentences.length === 0) return text.substring(0, 100);
  
  // Sort by length descending, pick the most substantial one
  sentences.sort((a, b) => b.length - a.length);
  // Take a middle-length sentence (not too short, not too long)
  const target = sentences[Math.floor(sentences.length / 3)] || sentences[0];
  
  // Clean and trim
  return target.replace(/[¬´¬ª""]/g, '').substring(0, 150).trim();
}

function calculateTextSimilarity(text1: string, text2: string): number {
  if (!text1 || !text2) return 0;
  
  const normalize = (t: string) => t.toLowerCase().replace(/[^–∞-—è—ëa-z0-9\s]/gi, '').split(/\s+/).filter(w => w.length > 3);
  const words1 = new Set(normalize(text1));
  const words2 = new Set(normalize(text2));
  
  if (words1.size === 0 || words2.size === 0) return 0;
  
  let intersection = 0;
  for (const word of words1) {
    if (words2.has(word)) intersection++;
  }
  
  // Jaccard similarity * 100
  const union = words1.size + words2.size - intersection;
  return Math.round((intersection / union) * 100);
}

function chunkText(text: string, chunkSize: number = 500): string[] {
  const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 50);
  const chunks: string[] = [];
  let current = '';
  
  for (const para of paragraphs) {
    if (current.length + para.length < chunkSize) {
      current += '\n\n' + para;
    } else {
      if (current.trim()) chunks.push(current.trim());
      current = para;
    }
  }
  if (current.trim()) chunks.push(current.trim());
  
  return chunks;
}

// ================== MAIN CHECK FUNCTION ==================

export async function checkPlagiarism(text: string, language: 'ru' | 'en' = 'ru'): Promise<PlagiarismResult> {
  logger.info(`[Plagiarism] Starting check: ${text.length} chars, language: ${language}`);
  
  const stats = analyzeTextStatistics(text);
  const chunks = chunkText(text);
  
  // 1. AI Pattern Detection (local, instant)
  const aiIssues = detectAIPatterns(text, language);
  
  // 2. Web search for duplicates (parallel)
  const [webSources, academicSources] = await Promise.all([
    searchWebForDuplicates(chunks),
    searchAcademicSources(chunks),
  ]);
  
  // 3. Combine and deduplicate sources
  const allSources = deduplicateSources([...webSources, ...academicSources]);
  
  // 4. Calculate scores
  const aiScore = calculateAIScore(aiIssues, stats);
  const plagiarismScore = calculatePlagiarismScore(allSources, stats.wordCount);
  const uniquenessScore = Math.max(0, Math.min(100, 100 - plagiarismScore));
  
  // 5. Build plagiarism issues from sources
  const plagiarismIssues: PlagiarismIssue[] = allSources
    .filter(s => s.similarity > 30)
    .map(s => ({
      type: 'plagiarism' as const,
      text: s.snippet.substring(0, 100),
      suggestion: `–°—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω: "${s.title}" (${s.similarity}% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)`,
      severity: s.similarity > 60 ? 'high' as const : s.similarity > 40 ? 'medium' as const : 'low' as const,
      source: s,
    }));
  
  // 6. Build strengths
  const strengths: string[] = [];
  if (stats.burstiness > 0.4) strengths.push('–•–æ—Ä–æ—à–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π');
  if (stats.ttr > 0.5) strengths.push('–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å');
  if (stats.hapaxRatio > 0.4) strengths.push('–ú–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤');
  if (aiIssues.length === 0) strengths.push('AI-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã');
  if (allSources.length === 0) strengths.push('–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ');
  if (stats.paragraphVariance > 5) strengths.push('–•–æ—Ä–æ—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å');
  if (strengths.length === 0) strengths.push('–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç');
  
  // 7. Build recommendations
  const recommendations: string[] = [];
  if (aiIssues.length > 5) recommendations.push('–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π—Ç–µ —Ç–∏–ø–∏—á–Ω—ã–µ AI-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏');
  if (stats.burstiness < 0.3) recommendations.push('–ß–µ—Ä–µ–¥—É–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è');
  if (stats.ttr < 0.4) recommendations.push('–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª—å—à–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤');
  if (allSources.length > 3) recommendations.push('–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π—Ç–µ —É—á–∞—Å—Ç–∫–∏, —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏');
  if (stats.sentenceStdDev < 4) recommendations.push('–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π');
  if (recommendations.length === 0) recommendations.push('–¢–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç —Ö–æ—Ä–æ—à–æ, –º–µ–ª–∫–∏–µ –ø—Ä–∞–≤–∫–∏ –ø–æ–≤—ã—Å—è—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å');
  
  // 8. Build summary
  const summaryParts: string[] = [];
  if (uniquenessScore >= 85) summaryParts.push('–í—ã—Å–æ–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞.');
  else if (uniquenessScore >= 65) summaryParts.push('–£–º–µ—Ä–µ–Ω–Ω–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.');
  else summaryParts.push('–ù–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å ‚Äî –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ —Å–µ—Ä—å—ë–∑–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞.');
  
  if (aiScore < 30) summaryParts.push('–ù–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.');
  else if (aiScore < 60) summaryParts.push('–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.');
  else summaryParts.push('–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≥—É–º–∞–Ω–∏–∑–∞—Ü–∏—è.');
  
  if (allSources.length > 0) summaryParts.push(`–ù–∞–π–¥–µ–Ω–æ ${allSources.length} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.`);
  
  const result: PlagiarismResult = {
    uniquenessScore,
    aiProbability: aiScore,
    sourcesFound: allSources.length,
    sources: allSources.slice(0, 10), // Limit to top 10
    analysis: {
      issues: [...plagiarismIssues, ...aiIssues].slice(0, 20),
      strengths,
      summary: summaryParts.join(' '),
    },
    recommendations,
    wordCount: stats.wordCount,
    characterCount: stats.characterCount,
    sentenceCount: stats.sentenceCount,
    metrics: {
      exactMatchPercent: Math.round(plagiarismScore * 0.6),
      paraphraseMatchPercent: Math.round(plagiarismScore * 0.4),
      citationCoverage: Math.min(100, Math.round((text.match(/\[\d+\]|\(\w+,\s*\d{4}\)/g)?.length || 0) * 10)),
      vocabularyRichness: Math.round(stats.ttr * 100),
      burstiness: Math.round(stats.burstiness * 100) / 100,
      perplexity: Math.round(stats.perplexity * 100) / 100,
    },
  };
  
  logger.info(`[Plagiarism] Check complete: uniqueness=${uniquenessScore}%, AI=${aiScore}%, sources=${allSources.length}`);
  return result;
}

// ================== SCORING ==================

function calculateAIScore(issues: PlagiarismIssue[], stats: ReturnType<typeof analyzeTextStatistics>): number {
  let score = 0;
  
  // Pattern-based scoring (max 50 points)
  const patternWeight = issues.reduce((sum, issue) => {
    return sum + (issue.severity === 'high' ? 6 : issue.severity === 'medium' ? 3 : 1);
  }, 0);
  score += Math.min(50, patternWeight * 2);
  
  // Statistical scoring (max 50 points)
  // Low burstiness = likely AI
  if (stats.burstiness < 0.2) score += 15;
  else if (stats.burstiness < 0.35) score += 8;
  else if (stats.burstiness < 0.45) score += 3;
  
  // Low TTR = limited vocabulary = likely AI
  if (stats.ttr < 0.35) score += 10;
  else if (stats.ttr < 0.45) score += 5;
  
  // Low hapax ratio = repetitive word use = likely AI
  if (stats.hapaxRatio < 0.3) score += 10;
  else if (stats.hapaxRatio < 0.4) score += 5;
  
  // Low paragraph variance = uniform structure = likely AI
  if (stats.paragraphVariance < 3) score += 8;
  else if (stats.paragraphVariance < 5) score += 4;
  
  // Low perplexity = predictable = likely AI
  if (stats.perplexity < 5) score += 7;
  else if (stats.perplexity < 10) score += 3;
  
  return Math.min(100, Math.max(0, score));
}

function calculatePlagiarismScore(sources: PlagiarismSource[], wordCount: number): number {
  if (sources.length === 0) return 0;
  
  // Average similarity of found sources, weighted by count
  const avgSimilarity = sources.reduce((sum, s) => sum + s.similarity, 0) / sources.length;
  const sourceFactor = Math.min(1, sources.length / 5); // More sources = higher score
  
  return Math.round(avgSimilarity * sourceFactor * 0.8); // Cap effective score
}

function deduplicateSources(sources: PlagiarismSource[]): PlagiarismSource[] {
  const seen = new Set<string>();
  return sources
    .sort((a, b) => b.similarity - a.similarity)
    .filter(s => {
      const key = s.url.replace(/^https?:\/\//, '').replace(/\/$/, '');
      if (seen.has(key)) return false;
      seen.add(key);
      return true;
    });
}
